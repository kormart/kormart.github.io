---
layout: post
title: "游젏릖 AI i mjukvarusystem / 游섫릖 AI in Software Systems"
date: 2026-02-15 12:00:00 -0000
categories: coding ai
---

There is a version in English below.

Det 칛r allt mer snack om att 24e november 2025 kommer r칛knas som ett historiskt datum. Det var d친 Anthropic sl칛ppte AI-modellen Opus 4.5. Och n칛r den anv칛nds tillsammans med agenten Claude Code passerades en gr칛ns, 친tminstone f칬r mjukvaruutveckling, d칛r kvaliten blev p친tagligt f칬rb칛ttrad. 

Men utveckling slutar inte d칛r. Hittills hj칛lper AI-agenter till med generering av k칛llkod som d칛refter granskas av m칛nniskor innan den f칛rdiga mjukvaran drifts칛tts. Det har 칛ven forskats mycket inom neurosymboliska metoder, d칛r fokus fortfarande 칛r p친 k칛llkodsniv친. Men kan AI-systemen f친 칛nnu mer direkt p친verkan p친 ett mjukvarusystem? Tv친 projekt tar olika v칛gar fram친t.

Nyckeln i b친da projekten 칛r att se bortom k칛llkoden och in i sj칛lva kodexekveringen (p친 engelska runtime), den dynamiska tillst친ndsmaskin d칛r v칛rden ber칛knas och lagras och kommunikation sker till och fr친n externa system. K칛llkoden 칛r s친 klart instruktionen f칬r denna exekvering. Men ofta finns icke-determinism i externa system och tidsutvecklingen av exekveringens tillst친nd blir mycket komplex. M칛nniskor som programmerar f칬rs칬ker ibland simulera detta genom att t칛nka p친 hur variablers v칛rden 칛ndras och d칛rmed uppt칛cka problem. Ibland anv칛nds avlusare (debuggers) som ger en direkt inblick i tillst친nden i kodexekveringen. Men m칛nniskor har begr칛nsningar i skalan av tillst친nd som kan h친llas i tanken. 

S친 hur kan en AI-agent f친r tillg친ng och p친verka kodexekveringen? Ett angreppss칛tt (som anv칛nds av startupbolget Symbolica i deras agentramverk Agentica) 칛r att l친ta AI-genererade kod-fragment inkluderas i en p친g친ende exekveringsmilj칬. Man kan s칛ga att nya objekt och funktioner (instanser d칛rav) sys ihop med de redan befintliga, s친 att funktionalitet och andra egenskaper kan 칛ndras dynamiskt. En utmaning 칛r att f친 typer och funktionssignaturer att passa ihop. Man lutar sig mot k칛nda metoder inom typteori och funktionell programmering (kategoriteori). En f칬rdel med detta angreppss칛tt 칛r att det 칬kar effektivitet och precision att lyfta ut delar av l칬sningen fr친n den generativa AI-modellen.    

Ett annat angreppss칛tt 칛r att modellera (representera) hela exekveringsmilj칬n i AI-modellens inre rymd, den latenta rymden. Ett projekt hos Metas FAIR-grupp, Code World Model (CWM), samlar ihop stora m칛ngder data fr친n tidsutvecklingar (sp친r) av kodexekvering, och g칬r f칬rst칛rkningsinl칛rning p친 detta. Syftet just nu 칛r att generera b칛ttre k칛llkod. Men man kan t칛nka sig i en f칬rl칛ngning att representationen av exekveringen blir tillr칛ckligt bra s친 att man inte beh칬ver k칬ra k칛llkoden. En m칬jlig f칬rdel med detta angreppss칛tt 칛r att det liknar andra dom칛ners World Model-skapande, tex de f칬r robotik och industriella system. Uppenbara nackdelar (ungef칛r samma som det v칛lk칛nda problemet med AI-modellers of칬rm친ga att r칛kna med stora tal) 칛r d친lig skalbarhet och effektivitet.

Denna utveckling skapar m칬jligheter f칬r mera autonomi och sj칛lv-f칬r칛ndrande (i b칛sta fall sj칛lv-f칬rb칛ttrande) egenskaper hos system. AI-agenter kan bli n칛rmare integrerade i produkter och processer. Det uppst친r s친 klart ocks친 m친nga fr친gor. F칬rutom att kvalitet och tillf칬rlitlighet m친ste bli tillr칛ckligt bra, hur kan m칛nniskor kontrollera, styra och f친 f칬rklarbarhet i dessa system?

Och varf칬r 칛r Claude Code med Opus 4.5/4.6 s친 mycket b칛ttre 칛n andra kod-agenter? F칬rmodligen har Anthropic anstr칛ngt sig mer 칛n andra f칬r att AI-modellen ska vara bra p친 just k칛llkod, tex genom post-tr칛ning-steg d칛r f칬rst칛rkningsinl칛rning (Reinforcement Learning) anv칛nds p친 testfr친gor d칛r man kan skapa verifierbara bel칬ningssignaler (Verifiable Rewards).


## Reading list

* Semianalysis, [Claude Code is the inflection point](https://newsletter.semianalysis.com/p/claude-code-is-the-inflection-point)
* Steve Yegge, [The AI Vampire](https://steve-yegge.medium.com/the-ai-vampire-eda6e4f07163)
* Symbolica / Agentica, [Beyond Code Mode](https://www.symbolica.ai/agentica)
* Code World Models, Meta FAIR, [arXiv:2510.02387](https://arxiv.org/abs/2510.02387)

Version in english

There's increasing talk that November 24, 2025 will be counted as a historic date. That's when Anthropic released the AI model Opus 4.5. And when used together with the Claude Code agent, a threshold was crossed, at least for software development, where the quality became noticeably improved.

But development doesn't stop there. Until now, AI agents have helped with source code generation which is then reviewed by humans before the finished software is deployed. There has also been much research within neurosymbolic methods, where the focus is still at the source code level. But can AI systems have even more direct impact on a software system? Two projects are taking different paths forward.

The key in both projects is to look beyond the source code and into the actual code execution (runtime), the dynamic state machine where values are calculated and stored and communication occurs to and from external systems. The source code is of course the instruction for this execution. But often there is non-determinism in external systems and the time evolution of the execution's state becomes very complex. Humans who program sometimes try to simulate this by thinking about how variable values change and thereby discover problems. Sometimes debuggers are used which give direct insight into the states in code execution. But humans have limitations in the scale of states that can be held in mind.

So how can an AI agent gain access to and affect code execution? One approach (used by the startup company Symbolica in their agent framework Agentica) is to let AI-generated code fragments be included in an ongoing execution environment. You could say that new objects and functions (instances thereof) are stitched together with those already existing, so that functionality and other properties can be changed dynamically. A challenge is getting types and function signatures to fit together. They lean on known methods from type theory and functional programming (category theory). An advantage of this approach is that it increases efficiency and precision to lift out parts of the solution from the generative AI model.

Another approach is to model (represent) the entire execution environment in the AI model's inner space, the latent space. A project at Meta's FAIR group, Code World Model (CWM), collects large amounts of data from time evolutions (traces) of code execution, and performs reinforcement learning on this. The purpose right now is to generate better source code. But one could imagine in an extension that the representation of execution becomes good enough that you don't need to run the source code. A possible advantage of this approach is that it resembles other domains' World Model creation, for example those for robotics and industrial systems. Obvious disadvantages (roughly the same as the well-known problem with AI models' inability to calculate with large numbers) are poor scalability and efficiency.

This development creates opportunities for more autonomy and self-modifying (in the best case self-improving) properties in systems. AI capabilities can become more closely integrated into products and processes. Of course, many questions also arise. Besides quality and reliability needing to become good enough, how can humans control, steer and achieve explainability in these systems?

And why is Claude Code with Opus 4.5/4.6 so much better than other code agents? Presumably Anthropic has made more effort than others to make the AI model good, specifically, at source code, for example through post-training steps where Reinforcement Learning is used on test questions where you can create Verifiable Rewards (RLVR).

